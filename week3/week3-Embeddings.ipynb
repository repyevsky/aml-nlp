{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find duplicate questions on StackOverflow by their embeddings\n",
    "\n",
    "In this assignment you will learn how to calculate a similarity for pieces of text. Using this approach you will know how to find duplicate questions from [StackOverflow](https://stackoverflow.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries\n",
    "\n",
    "In this task you will you will need the following libraries:\n",
    "- [StarSpace](https://github.com/facebookresearch/StarSpace) — a general-purpose model for efficient learning of entity embeddings from Facebook\n",
    "- [Gensim](https://radimrehurek.com/gensim/) — a tool for solving various NLP-related tasks (topic modeling, text representation, ...)\n",
    "- [Numpy](http://www.numpy.org) — a package for scientific computing.\n",
    "- [scikit-learn](http://scikit-learn.org/stable/index.html) — a tool for data mining and data analysis.\n",
    "- [Nltk](http://www.nltk.org) — a platform to work with human language data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "The following cell will download all data required for this assignment into the folder `week3/data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from common.download_utils import download_week3_resources\n",
    "\n",
    "# download_week3_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading\n",
    "We will create a grader instace below and use it to collect your answers. Note that these outputs will be stored locally inside grader and will be uploaded to platform only after running submiting function in the last part of this assignment. If you want to make partial submission, you can run that cell any time you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grader import Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader = Grader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embedding\n",
    "\n",
    "To solve the problem, you will use two different models of embeddings:\n",
    "\n",
    " - [Pre-trained word vectors](https://code.google.com/archive/p/word2vec/) from Google which were trained on a part of Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases. `GoogleNews-vectors-negative300.bin.gz` will be downloaded in `download_week3_resources()`.\n",
    " - Representations using StarSpace on StackOverflow data sample. You will need to train them from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's always easier to start with pre-trained embeddings. Unpack the pre-trained Goggle's vectors and upload them using the function [KeyedVectors.load_word2vec_format](https://radimrehurek.com/gensim/models/keyedvectors.html) from gensim library with the parameter *binary=True*. If the size of the embeddings is larger than the avaliable memory, you could load only a part of the embeddings by defining the parameter *limit* (recommended: 500000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_embeddings = gensim.models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True,\n",
    "                                                                limit=500000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to work with Google's word2vec embeddings?\n",
    "\n",
    "Once you have loaded the representations, make sure you can access them. First, you can check if the loaded embeddings contain a word:\n",
    "    \n",
    "    'word' in wv_embeddings\n",
    "    \n",
    "Second, to get the corresponding embedding you can use the square brackets:\n",
    "\n",
    "    wv_embeddings['word']\n",
    " \n",
    "### Checking that the embeddings are correct \n",
    " \n",
    "To prevent any errors during the first stage, we can check that the loaded embeddings are correct. You can call the function *check_embeddings*, implemented below, which runs 3 tests:\n",
    "1. Find the most similar word for provided \"positive\" and \"negative\" words.\n",
    "2. Find which word from the given list doesn’t go with the others.\n",
    "3. Find the most similar word for the provided one.\n",
    "\n",
    "In the right case the function will return the string *These embeddings look good*. Othervise, you need to validate the previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def check_embeddings(embeddings):\n",
    "    error_text = \"Something wrong with your embeddings ('%s test isn't correct).\"\n",
    "    most_similar = embeddings.most_similar(positive=['woman', 'king'], negative=['man'])\n",
    "    if len(most_similar) < 1 or most_similar[0][0] != 'queen':\n",
    "        return error_text % \"Most similar\"\n",
    "\n",
    "    doesnt_match = embeddings.doesnt_match(['breakfast', 'cereal', 'dinner', 'lunch'])\n",
    "    if doesnt_match != 'cereal':\n",
    "        return error_text % \"Doesn't match\"\n",
    "    \n",
    "    most_similar_to_given = embeddings.most_similar_to_given('music', ['water', 'sound', 'backpack', 'mouse'])\n",
    "    if most_similar_to_given != 'sound':\n",
    "        return error_text % \"Most similar to given\"\n",
    "    \n",
    "    return \"These embeddings look good.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These embeddings look good.\n"
     ]
    }
   ],
   "source": [
    "print(check_embeddings(wv_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From word to text embeddings\n",
    "\n",
    "**Task 1 (Question2Vec).** Usually, we have word-based embeddings, but for the task we need to create a representation for the whole question. It could be done in different ways. In our case we will use a **mean** of all word vectors in the question. Now you need to implement the function *question_to_vec*, which calculates the question representation described above. This function should work with the input text as is without any preprocessing.\n",
    "\n",
    "Note that there could be words without the corresponding embeddings. In this case, you can just skip these words and don't take them into account during calculating the result. If the question doesn't contain any known word with embedding, the function should return a zero vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_to_vec(question, embeddings, dim=300):\n",
    "    \"\"\"\n",
    "        question: a string\n",
    "        embeddings: dict where the key is a word and a value is its' embedding\n",
    "        dim: size of the representation\n",
    "\n",
    "        result: vector representation for the question\n",
    "    \"\"\"\n",
    "    ######################################\n",
    "    ######### YOUR CODE HERE #############\n",
    "    ######################################\n",
    "    embedding = np.zeros(dim, dtype=np.float32)\n",
    "    found_words = 0\n",
    "    for word in question.split():\n",
    "        if word in embeddings:\n",
    "            embedding += embeddings[word]\n",
    "            found_words += 1\n",
    "    return embedding / max(found_words, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the basic correctness of your implementation, run the function *question_to_vec_tests*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_to_vec_tests():\n",
    "    if (np.zeros(300) != question_to_vec('', wv_embeddings)).any():\n",
    "        return \"You need to return zero vector for empty question.\"\n",
    "    if (np.zeros(300) != question_to_vec('thereisnosuchword', wv_embeddings)).any():\n",
    "        return \"You need to return zero vector for the question, which consists only unknown words.\"\n",
    "    if (wv_embeddings['word'] != question_to_vec('word', wv_embeddings)).any():\n",
    "        return \"You need to check the corectness of your function.\"\n",
    "    if ((wv_embeddings['I'] + wv_embeddings['am']) / 2 != question_to_vec('I am', wv_embeddings)).any():\n",
    "        return \"Your function should calculate a mean of word vectors.\"\n",
    "    if (wv_embeddings['word'] != question_to_vec('thereisnosuchword word', wv_embeddings)).any():\n",
    "        return \"You should not consider words which embeddings are unknown.\"\n",
    "    return \"Basic tests are passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic tests are passed.\n"
     ]
    }
   ],
   "source": [
    "print(question_to_vec_tests())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can submit embeddings for the questions from the file *test_embeddings.tsv* to earn the points. In this task you don't need to transform the text of a question somehow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from util import array_to_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task Question2Vec is: 0.019293891266\n",
      "-0.0287272129208\n",
      "0.0460561104119\n",
      "0.0852593332529\n",
      "0.0243055559695\n",
      "-0.0729031041265\n",
      "0.0...\n"
     ]
    }
   ],
   "source": [
    "question2vec_result = []\n",
    "for question in open('data/test_embeddings.tsv'):\n",
    "    question = question.strip()\n",
    "    answer = question_to_vec(question, wv_embeddings)\n",
    "    question2vec_result = np.append(question2vec_result, answer)\n",
    "\n",
    "grader.submit_tag('Question2Vec', array_to_string(question2vec_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a method to create a representation of any sentence and we are ready for the first evaluation. So, let's check how well our solution (Google's vectors + *question_to_vec*) will work.\n",
    "\n",
    "## Evaluation of text similarity\n",
    "\n",
    "We can imagine that if we use good embeddings, the cosine similarity between the duplicate sentences should be less than for the random ones. Overall, for each pair of duplicate sentences we can generate *R* random negative examples and find out the position of the correct duplicate.  \n",
    "\n",
    "For example, we have the question *\"Exceptions What really happens\"* and we are sure that another question *\"How does the catch keyword determine the type of exception that was thrown\"* is a duplicate. But our model doesn't know it and tries to find out the best option also among questions like *\"How Can I Make These Links Rotate in PHP\"*, *\"NSLog array description not memory address\"* and *\"PECL_HTTP not recognised php ubuntu\"*. The goal of the model is to rank all these 4 questions (1 *positive* and *R* = 3 *negative*) in the way that the correct one is in the first place.\n",
    "\n",
    "However, it is unnatural to count on that the best candidate will be always in the first place. So let us consider the place of the best candidate in the sorted list of candidates and formulate a metric based on it. We can fix some *K* — a reasonalble number of top-ranked elements and *N* — a number of queries (size of the sample).\n",
    "\n",
    "### Hits@K\n",
    "\n",
    "The first simple metric will be a number of correct hits for some *K*:\n",
    "$$ \\text{Hits@K} = \\frac{1}{N}\\sum_{i=1}^N \\, [dup_i \\in topK(q_i)]$$\n",
    "\n",
    "where $q_i$ is the i-th query, $dup_i$ is its duplicate, $topK(q_i)$ is the top K elements of the ranked sentences provided by our model and the operation $[dup_i \\in topK(q_i)]$ equals 1 if the condition is true and 0 otherwise (more details about this operation could be found [here](https://en.wikipedia.org/wiki/Iverson_bracket)).\n",
    "\n",
    "\n",
    "### DCG@K\n",
    "The second one is a simplified [DCG metric](https://en.wikipedia.org/wiki/Discounted_cumulative_gain):\n",
    "\n",
    "$$ \\text{DCG@K} = \\frac{1}{N} \\sum_{i=1}^N\\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le K] $$\n",
    "\n",
    "where $rank_{dup_i}$ is a position of the duplicate in the sorted list of the nearest sentences for the query $q_i$. According to this metric, the model gets a higher reward for a higher position of the correct answer. If the answer does not appear in topK at all, the reward is zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation examples\n",
    "\n",
    "Let's calculate the described metrics for the toy example introduced above. In this case $N$ = 1 and the correct candidate for $q_1$ is *\"How does the catch keyword determine the type of exception that was thrown\"*. Consider the following ranking of the candidates:\n",
    "1. *\"How Can I Make These Links Rotate in PHP\"*\n",
    "2. *\"How does the catch keyword determine the type of exception that was thrown\"*\n",
    "3. *\"NSLog array description not memory address\"*\n",
    "4. *\"PECL_HTTP not recognised php ubuntu\"*\n",
    "\n",
    "Using the ranking above, calculate *Hits@K* metric for *K = 1, 2, 4*: \n",
    " \n",
    "- [K = 1] $\\text{Hits@1} = \\frac{1}{1}\\sum_{i=1}^1 \\, [dup_i \\in top1(q_i)] = [dup_1 \\in top1(q_1)] = 0$ because the correct answer doesn't appear in the *top1* list.\n",
    "- [K = 2] $\\text{Hits@2} = \\frac{1}{1}\\sum_{i=1}^1 \\, [dup_i \\in top2(q_i)] = [dup_1 \\in top2(q_1)] = 1$ because $rank_{dup_1} = 2$.\n",
    "- [K = 4] $\\text{Hits@4} = \\frac{1}{1}\\sum_{i=1}^1 \\, [dup_i \\in top4(q_i)] = [dup_1 \\in top4(q_1)] = 1$\n",
    "\n",
    "Using the ranking above, calculate *DCG@K* metric for *K = 1, 2, 4*:\n",
    "\n",
    "- [K = 1] $\\text{DCG@1} = \\frac{1}{1} \\sum_{i=1}^1\\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le 1] = \\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le 1] = 0$ because the correct answer doesn't appear in the top1 list.\n",
    "- [K = 2] $\\text{DCG@2} = \\frac{1}{1} \\sum_{i=1}^1\\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le 2] = \\frac{1}{\\log_2{3}}$, because $rank_{dup_1} = 2$.\n",
    "- [K = 4] $\\text{DCG@4} = \\frac{1}{1} \\sum_{i=1}^1\\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le 4] = \\frac{1}{\\log_2{3}}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks 2 and 3 (HitsCount and DCGScore).** Implement the functions *hits_count* and *dcg_score* as described above. Each function has two arguments: *dup_ranks* and *k*. *dup_ranks* is a list which contains *values of ranks* of duplicates. For example, *dup_ranks* is *[2]* for the example provided above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hits_count(dup_ranks, k):\n",
    "    \"\"\"\n",
    "        dup_ranks: list of duplicates' ranks; one rank per question; \n",
    "                   length is a number of questions which we are looking for duplicates; \n",
    "                   rank is a number from 1 to len(candidates of the question); \n",
    "                   e.g. [2, 3] means that the first duplicate has the rank 2, the second one — 3.\n",
    "        k: number of top-ranked elements (k in Hits@k metric)\n",
    "\n",
    "        result: return Hits@k value for current ranking\n",
    "    \"\"\"\n",
    "    ######################################\n",
    "    ######### YOUR CODE HERE #############\n",
    "    ######################################\n",
    "    return (np.array(dup_ranks) <= k).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your code on the tiny examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_hits():\n",
    "    # *Evaluation example*\n",
    "    # answers — dup_i\n",
    "    answers = [\"How does the catch keyword determine the type of exception that was thrown\"]\n",
    "    \n",
    "    # candidates_ranking — the ranked sentences provided by our model\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\",\n",
    "                           \"NSLog array description not memory address\",\n",
    "                           \"PECL_HTTP not recognised php ubuntu\"]]\n",
    "    # dup_ranks — position of the dup_i in the list of ranks +1\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    \n",
    "    # correct_answers — the expected values of the result for each k from 1 to 4\n",
    "    correct_answers = [0, 1, 1, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(hits_count(dup_ranks, k), correct):\n",
    "            # from IPython.core.debugger import Tracer; Tracer()() \n",
    "            return \"Check the function.\"\n",
    "    \n",
    "    # Other tests\n",
    "    answers = [\"How does the catch keyword determine the type of exception that was thrown\", \n",
    "               \"Convert Google results object (pure js) to Python object\"]\n",
    "    \n",
    "    # The first test: both duplicates on the first position in ranked list\n",
    "    candidates_ranking = [[\"How does the catch keyword determine the type of exception that was thrown\",\n",
    "                           \"How Can I Make These Links Rotate in PHP\"], \n",
    "                          [\"Convert Google results object (pure js) to Python object\",\n",
    "                           \"WPF- How to update the changes in list item of a list\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [1, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        # from IPython.core.debugger import Tracer; Tracer()() \n",
    "        if not np.isclose(hits_count(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: both duplicates on the first position in ranked list).\"\n",
    "        \n",
    "    # The second test: one candidate on the first position, another — on the second\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\"], \n",
    "                          [\"Convert Google results object (pure js) to Python object\",\n",
    "                           \"WPF- How to update the changes in list item of a list\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [0.5, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(hits_count(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: one candidate on the first position, another — on the second).\"\n",
    "\n",
    "    # The third test: both candidates on the second position\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\"], \n",
    "                          [\"WPF- How to update the changes in list item of a list\",\n",
    "                           \"Convert Google results object (pure js) to Python object\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [0, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(hits_count(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: both candidates on the second position).\"\n",
    "\n",
    "    return \"Basic test are passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic test are passed.\n"
     ]
    }
   ],
   "source": [
    "print(test_hits())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_score(dup_ranks, k):\n",
    "    \"\"\"\n",
    "        dup_ranks: list of duplicates' ranks; one rank per question; \n",
    "                   length is a number of questions which we are looking for duplicates; \n",
    "                   rank is a number from 1 to len(candidates of the question); \n",
    "                   e.g. [2, 3] means that the first duplicate has the rank 2, the second one — 3.\n",
    "        k: number of top-ranked elements (k in DCG@k metric)\n",
    "\n",
    "        result: return DCG@k value for current ranking\n",
    "    \"\"\"\n",
    "    ######################################\n",
    "    ######### YOUR CODE HERE #############\n",
    "    ######################################\n",
    "    dup_ranks = np.array(dup_ranks)\n",
    "    is_in_top_k = dup_ranks <= k\n",
    "    return (is_in_top_k / np.log2(1.0 + dup_ranks)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dcg():\n",
    "    # *Evaluation example*\n",
    "    # answers — dup_i\n",
    "    answers = [\"How does the catch keyword determine the type of exception that was thrown\"]\n",
    "    \n",
    "    # candidates_ranking — the ranked sentences provided by our model\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\",\n",
    "                           \"NSLog array description not memory address\",\n",
    "                           \"PECL_HTTP not recognised php ubuntu\"]]\n",
    "    # dup_ranks — position of the dup_i in the list of ranks +1\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    \n",
    "    # correct_answers — the expected values of the result for each k from 1 to 4\n",
    "    correct_answers = [0, 1 / (np.log2(3)), 1 / (np.log2(3)), 1 / (np.log2(3))]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(dcg_score(dup_ranks, k), correct):\n",
    "            return \"Check the function.\"\n",
    "    \n",
    "    # Other tests\n",
    "    answers = [\"How does the catch keyword determine the type of exception that was thrown\", \n",
    "               \"Convert Google results object (pure js) to Python object\"]\n",
    "\n",
    "    # The first test: both duplicates on the first position in ranked list\n",
    "    candidates_ranking = [[\"How does the catch keyword determine the type of exception that was thrown\",\n",
    "                           \"How Can I Make These Links Rotate in PHP\"], \n",
    "                          [\"Convert Google results object (pure js) to Python object\",\n",
    "                           \"WPF- How to update the changes in list item of a list\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [1, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(dcg_score(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: both duplicates on the first position in ranked list).\"\n",
    "        \n",
    "    # The second test: one candidate on the first position, another — on the second\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\"], \n",
    "                          [\"Convert Google results object (pure js) to Python object\",\n",
    "                           \"WPF- How to update the changes in list item of a list\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [0.5, (1 + (1 / (np.log2(3)))) / 2]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(dcg_score(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: one candidate on the first position, another — on the second).\"\n",
    "        \n",
    "    # The third test: both candidates on the second position\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\",\n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\"], \n",
    "                          [\"WPF- How to update the changes in list item of a list\",\n",
    "                           \"Convert Google results object (pure js) to Python object\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [0, 1 / (np.log2(3))]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(dcg_score(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: both candidates on the second position).\"\n",
    "\n",
    "    return \"Basic test are passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic test are passed.\n"
     ]
    }
   ],
   "source": [
    "print(test_dcg())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit results of the functions *hits_count* and *dcg_score* for the following examples to earn the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_examples = [\n",
    "    [1],\n",
    "    [1, 2],\n",
    "    [2, 1],\n",
    "    [1, 2, 3],\n",
    "    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    [9, 5, 4, 2, 8, 10, 7, 6, 1, 3],\n",
    "    [4, 3, 5, 1, 9, 10, 7, 8, 2, 6],\n",
    "    [5, 1, 7, 6, 2, 3, 8, 9, 10, 4],\n",
    "    [6, 3, 1, 4, 7, 2, 9, 8, 10, 5],\n",
    "    [10, 9, 8, 7, 6, 5, 4, 3, 2, 1],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task HitsCount is: 1.0\n",
      "0.5\n",
      "1.0\n",
      "0.5\n",
      "1.0\n",
      "0.333333333333\n",
      "0.666666666667\n",
      "1.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "1.0\n",
      "0.1\n",
      "0....\n"
     ]
    }
   ],
   "source": [
    "hits_results = []\n",
    "for example in test_examples:\n",
    "    for k in range(len(example)):\n",
    "        hits_results.append(hits_count(example, k + 1))\n",
    "grader.submit_tag('HitsCount', array_to_string(hits_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task DCGScore is: 1.0\n",
      "0.5\n",
      "0.815464876786\n",
      "0.5\n",
      "0.815464876786\n",
      "0.333333333333\n",
      "0.54364325119\n",
      "0.710309917857\n",
      "0.1\n",
      "0.16309297...\n"
     ]
    }
   ],
   "source": [
    "dcg_results = []\n",
    "for example in test_examples:\n",
    "    for k in range(len(example)):\n",
    "        dcg_results.append(dcg_score(example, k + 1))\n",
    "grader.submit_tag('DCGScore', array_to_string(dcg_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  First solution: pre-trained embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work with predefined train, validation and test corpora. All the files are tab-separated, but have a different format:\n",
    " - *train* corpus contains similar sentences at the same row.\n",
    " - *validation* corpus contains the following columns: *question*, *similar question*, *negative example 1*, *negative example 2*, ... \n",
    " - *test* corpus contains the following columns: *question*, *example 1*, *example 2*, ...\n",
    "\n",
    "Validation corpus will be used for the intermediate validation of models. The test data will be necessary for submitting the quality of your model in the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should upload *validation* corpus to evaluate current solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(filename):\n",
    "    data = []\n",
    "    for line in open(filename, encoding='utf-8'):\n",
    "        data.append(line.strip().split('\\t'))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = read_corpus('data/validation.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use cosine distance to rank candidate questions which you need to implement in the function *rank_candidates*. The function should return a sorted list of pairs *(initial position in candidates list, candidate)*. Index of some pair corresponds to its rank (the first is the best). For example, if the list of candidates was *[a, b, c]* and the most similar is *c*, then *a* and *b*, the function should return a list *[(2, c), (0, a), (1, b)]*.\n",
    "\n",
    "Pay attention, if you use the function *cosine_similarity* from *sklearn.metrics.pairwise* to calculate similarity because it works in a different way: most similar objects has greatest similarity. It's preferable to use a vectorized version of *cosine_similarity* function. Try to compute similarity at once and not use list comprehension. It should speed up your computations significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def rank_candidates(question, candidates, embeddings, dim=300):\n",
    "    \"\"\"\n",
    "        question: a string\n",
    "        candidates: a list of strings (candidates) which we want to rank\n",
    "        embeddings: some embeddings\n",
    "        dim: dimension of the current embeddings\n",
    "        \n",
    "        result: a list of pairs (initial position in the list, question)\n",
    "    \"\"\"\n",
    "    \n",
    "    ######################################\n",
    "    ######### YOUR CODE HERE #############\n",
    "    ######################################\n",
    "    question_embedding = question_to_vec(question, embeddings, dim=dim)\n",
    "    similarity = cosine_similarity(question_embedding.reshape(1,-1), [question_to_vec(c, embeddings) for c in candidates]).reshape(-1)\n",
    "    return [(idx, candidates[idx]) for idx in np.argsort(similarity)[::-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your code on the tiny examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rank_candidates():\n",
    "    questions = ['converting string to list', 'Sending array via Ajax fails']\n",
    "    candidates = [['Convert Google results object (pure js) to Python object', \n",
    "                   'C# create cookie from string and send it',\n",
    "                   'How to use jQuery AJAX for an outside domain?'], \n",
    "                  ['Getting all list items of an unordered list in PHP', \n",
    "                   'WPF- How to update the changes in list item of a list', \n",
    "                   'select2 not displaying search results']]\n",
    "    results = [[(1, 'C# create cookie from string and send it'), \n",
    "                (0, 'Convert Google results object (pure js) to Python object'), \n",
    "                (2, 'How to use jQuery AJAX for an outside domain?')],\n",
    "               [(0, 'Getting all list items of an unordered list in PHP'), \n",
    "                (2, 'select2 not displaying search results'), \n",
    "                (1, 'WPF- How to update the changes in list item of a list')]]\n",
    "    for question, q_candidates, result in zip(questions, candidates, results):\n",
    "        ranks = rank_candidates(question, q_candidates, wv_embeddings, 300)\n",
    "        if not np.all(ranks == result):\n",
    "            return \"Check the function.\"\n",
    "    return \"Basic tests are passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic tests are passed.\n"
     ]
    }
   ],
   "source": [
    "print(test_rank_candidates())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test the quality of the current approach. Run the next two cells to get the results. Pay attention that calculation of similarity between vectors takes time and this calculation is computed approximately in 10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_ranking = []\n",
    "for line in validation:\n",
    "    q, *ex = line\n",
    "    ranks = rank_candidates(q, ex, wv_embeddings)\n",
    "    wv_ranking.append([r[0] for r in ranks].index(0) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.209 | Hits@   1: 0.209\n",
      "DCG@   5: 0.263 | Hits@   5: 0.311\n",
      "DCG@  10: 0.279 | Hits@  10: 0.360\n",
      "DCG@ 100: 0.316 | Hits@ 100: 0.548\n",
      "DCG@ 500: 0.349 | Hits@ 500: 0.807\n",
      "DCG@1000: 0.369 | Hits@1000: 1.000\n"
     ]
    }
   ],
   "source": [
    "for k in [1, 5, 10, 100, 500, 1000]:\n",
    "    print(\"DCG@%4d: %.3f | Hits@%4d: %.3f\" % (k, dcg_score(wv_ranking, k), k, hits_count(wv_ranking, k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did all the steps correctly, you should be frustrated by the received results. Let's try to understand why the quality is so low. First of all, when you work with some data it is necessary to have an idea how the data looks like. Print several questions from the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to print a binary heap tree without recursion? How do you best convert a recursive function to an iterative one? How can i use ng-model with directive in angular js flash: drawing and erasing\n",
      "How to start PhoneStateListener programmatically? PhoneStateListener and service Java cast object[] to model WCF and What does this mean?\n",
      "jQuery: Show a div2 when mousenter over div1 is over when hover on div1 depenting on if it is on div2 or not it should act differently How to run selenium in google app engine/cloud? Python Comparing two lists of strings for similarities\n"
     ]
    }
   ],
   "source": [
    "for line in validation[:3]:\n",
    "    q, *examples = line\n",
    "    print(q, *examples[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we deal with the raw data. It means that we have many punctuation marks, special characters and unlowercased letters. In our case, it could lead to the situation where we can't find some embeddings, e.g. for the word \"grid?\". \n",
    "\n",
    "To solve this problem you should use the functions *text_prepare* from the previous assignments to prepare the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import text_prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now transform all the questions from the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aasd'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_prepare('Aasd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_validation = []\n",
    "for line in validation:\n",
    "    prepared_validation.append([text_prepare(text) for text in line])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the approach again after the preparation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_prepared_ranking = []\n",
    "for line in prepared_validation:\n",
    "    q, *ex = line\n",
    "    ranks = rank_candidates(q, ex, wv_embeddings)\n",
    "    wv_prepared_ranking.append([r[0] for r in ranks].index(0) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.305 | Hits@   1: 0.305\n",
      "DCG@   5: 0.375 | Hits@   5: 0.438\n",
      "DCG@  10: 0.392 | Hits@  10: 0.489\n",
      "DCG@ 100: 0.425 | Hits@ 100: 0.656\n",
      "DCG@ 500: 0.447 | Hits@ 500: 0.830\n",
      "DCG@1000: 0.465 | Hits@1000: 1.000\n"
     ]
    }
   ],
   "source": [
    "for k in [1, 5, 10, 100, 500, 1000]:\n",
    "    print(\"DCG@%4d: %.3f | Hits@%4d: %.3f\" % (k, dcg_score(wv_prepared_ranking, k), \n",
    "                                              k, hits_count(wv_prepared_ranking, k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, prepare also train and test data, because you will need it in the future:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_file(in_, out_):\n",
    "    out = open(out_, 'w')\n",
    "    for line in open(in_, encoding='utf8'):\n",
    "        line = line.strip().split('\\t')\n",
    "        new_line = [text_prepare(q) for q in line]\n",
    "        print(*new_line, sep='\\t', file=out)\n",
    "    out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('data/test.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How to access a structure member without knowing the name?\\tGWT Servlet - ClassNotFoundException\\tSlowCheetah not transforming for EntityFramework Migrations\\tDevelopment tools permission I did not set\\tGo to scene without using button in Flash ASC3\\tobjective-c get last 2 characters of a string?\\tInserting a rails link into a Google Maps infowindow\\tWhen and where do I require files in a rails application?\\tdo-while loop in R\\tXerces-C++ DOM node line/column number location\\tPython, NetCDF4 and HDF5\\tGenerating a MD5 Hash with Qt\\txsl scenario when a node is not present in every for each loop\\tAbsolute beginners guide to working with audio in C/C++?\\tNo description or item name on PayPal Payment Received email\\tLaTeX annotation in MATLAB\\tReplace slashes between double quote characters using sed\\tiostream library not included in codeblocks\\tSafe Process.Start implementation for untrusted URL strings\\tTrouble linking php to database\\tGoogle Analytics Api Error\\tpossible to return only one column using JPA\\tNeed htaccess redirection\\tHow to get a full description of an activity in cleartool?\\tCrystal Reports Cutting Off Text in PDF\\tphantomjs on Mac OS X works from the command line, not via exec()\\tHow do I \"diff\" multiple files against a single base file?\\tWhy should I remove script tags from the DOM?\\tLiferay Login page redirect\\tWriting DataAnnotations best approach\\t.NET Claim in three namespaces\\tHow to return value from function in color?\\tDataTable filter with Date range\\tLayout For SQLite Resuts\\tandroid calendar delete event\\tSending Email via GMail and .NET 4\\tVim regex searching: alter placement of cursor in matched strings\\tWordPress REST API plugin get slow response\\tBroken Windows-Android USB-Connection between Device and PC\\tAre pointers primitive types in C++?\\tAngularFire - impossible to recover the time entered by the user in database\\tC program crashes when a function is called.\\tis there a cleaner way to right operator[]() for a vector?\\tStream Youtube video in MediaElement - Windows Store App\\tHow do I check if one vector is a subset of another?\\tHow to add a Dictionary<string,List<string>> to a DataTable in C#?\\tCheck whether an object is ObservableCollection or not?\\twhy doesnt this timer work\\tSpring data elastic search sort by document types\\thow to determine the available physical memory in linux\\tHow to check multiple values to be unique in CodeIgniter\\tWas OpenSagres allowed to use Apache POI-like packages?\\tWhy does a UIColor\\'s values change after serialization/deserialization?\\tUnable to read checkbox in a GridView\\tC++ and Java Communication for image processing application\\tNothing showing in the installation field parse.com obj-c\\tUser Input Restriction goes haywire\\tEmail with Node.js and Express.js and AngularJs and nodemiailer not sending mail\\tc# Serialize a class that has a list of elements of the same type as the class\\tfind minDistance parameter in requestlocationupdates is used?\\tcalling a VB.net function from javascript\\tMinimize function selection and function call overhead?\\tDataGridView not showing Columns/Data\\tDisable context menu in Internet Explorer control\\tBest way for simple game-loop in Javascript?\\tSound of App running on iPhone too low\\tUsing Phoenix with Cloudera Hbase (installed from repo)\\tWeb Crawler not working in nested divs\\tImporting another module from another subdirectory of the current directory\\'s parent directory (python)\\tcalculus engine in vb?\\tiPad: problems with table rows in popovercontrol\\tIs there a way to make a cell editable where column is readonly in XtraGrid?\\tPull down UIScrollView and hold y position\\tHow to write Iron-router nested template?\\tRaphael paper editing\\tProblem with jQuery edit-in-place with live() function.. need a ninja\\tSSRS using iff when dividing\\tApply and lubridate\\tCakePHP 2.0 cannot save\\tCUDA: injecting my own PTX function?\\tExcel Script Hyperlink Extraction\\tSequential image presentation in Pygame\\tTesting a Web application using AngularJS: MVC implementation\\tPushing a navigation controller is not supported in iOS 7.1, but works fine in iOS 8\\tSorting array(NSString + Number) using NSSortDescriptor in IOS?\\tLarge Photo Version from contacts in android\\tGCE managed groups api for horizontally scaling kubernetes nodes\\tMethod for sorting Arabic words alphabetically ios7\\tDelphi Prism: How to load Winform without showing it?\\tCan\\'t solve add(android.support.v4.app.Fragment) in List can not be applied to (make.application.Fragment)\\tCan controller names in RESTful routes be optional?\\tPlus 1 button doesn\\'t render properly in IE9\\tMarkdown live preview in Textarea?\\tCreating a new linear layout with specific paramaters inside another linear layout\\tInclude prebuilt static library on existing aosp module\\tWhat is the correct way to query Hive on Spark for maximum performance?\\tAdding pagination to a repeater\\tCan multiple inputs each using typeahead.js with different sources be consolidated?\\tUINavigationController back button doesn\\'t pop to previous VC\\thow to pass a variable in WHERE IN clause of oracle sql?\\tDoctrine - preUpdate invokes but doesn\\'t change a related entity property while prePersist does\\n'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_file('data/test.tsv', 'data/test_prepared.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "######### YOUR CODE HERE #############\n",
    "######################################\n",
    "prepare_file('data/train.tsv', 'data/train_prepared.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4 (W2VTokenizedRanks).** For each question from prepared *test.tsv* submit the ranks of the candidates to earn the points. The calculations should take about 3-5 minutes. Pay attention that the function *rank_candidates* returns a ranking, while in this case you should find a position in this ranking. Ranks should start with 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import matrix_to_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task W2VTokenizedRanks is: 95\t94\t7\t9\t64\t36\t31\t93\t23\t100\t99\t20\t60\t6\t97\t48\t70\t37\t41\t96\t29\t56\t2\t65\t68\t44\t27\t25\t57\t62\t11\t87\t50\t66\t7...\n"
     ]
    }
   ],
   "source": [
    "w2v_ranks_results = []\n",
    "prepared_test_data = 'data/test_prepared.tsv'\n",
    "for line in open(prepared_test_data):\n",
    "    q, *ex = line.strip().split('\\t')\n",
    "    ranks = rank_candidates(q, ex, wv_embeddings, 300)\n",
    "    ranked_candidates = [r[0] for r in ranks]\n",
    "    w2v_ranks_results.append([ranked_candidates.index(i) + 1 for i in range(len(ranked_candidates))])\n",
    "    \n",
    "grader.submit_tag('W2VTokenizedRanks', matrix_to_string(w2v_ranks_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced solution: StarSpace embeddings\n",
    "\n",
    "Now you are ready to train your own word embeddings! In particular, you need to train embeddings specially for our task of duplicates detection. Unfortunately, StarSpace cannot be run on Windows and we recommend to use provided\n",
    "[docker container](https://github.com/hse-aml/natural-language-processing/blob/master/Docker-tutorial.md) or other alternatives. Don't delete results of this task because you will need it in the final project.\n",
    "\n",
    "### How it works and what's the main difference with word2vec?\n",
    "The main point in this section is that StarSpace can be trained specifically for some tasks. In contrast to word2vec model, which tries to train similar embeddings for words in similar contexts, StarSpace uses embeddings for the whole sentence (just as a sum of embeddings of words and phrases). Despite the fact that in both cases we get word embeddings as a result of the training, StarSpace embeddings are trained using some supervised data, e.g. a set of similar sentence pairs, and thus they can better suit the task.\n",
    "\n",
    "In our case, StarSpace should use two types of sentence pairs for training: \"positive\" and \"negative\". \"Positive\" examples are extracted from the train sample (duplicates, high similarity) and the \"negative\" examples are generated randomly (low similarity assumed). \n",
    "\n",
    "### How to choose the best params for the model?\n",
    "Normally, you would start with some default choice and then run extensive experiments to compare different strategies. However, we have some recommendations ready for you to save your time:\n",
    "- Be careful with choosing the suitable training mode. In this task we want to explore texts similarity which corresponds to *trainMode = 3*.\n",
    "- Use adagrad optimization (parameter *adagrad = true*).\n",
    "- Set the length of phrase equal to 1 (parameter *ngrams*), because we need embeddings only for words.\n",
    "- Don't use a large number of *epochs* (we think that 5 should be enough).\n",
    "- Try dimension *dim* equal to 100.\n",
    "- To compare embeddings usually *cosine* *similarity* is used.\n",
    "- Set *minCount* greater than 1 (for example, 2) if you don't want to get embeddings for extremely rare words.\n",
    "- Parameter *verbose = true* could show you the progress of the training process.\n",
    "- Set parameter *fileFormat* equals *labelDoc*.\n",
    "- Parameter *negSearchLimit* is responsible for a number of negative examples which is used during the training. We think that 10 will be enought for this task.\n",
    "- To increase a speed of training we recommend to set *learning rate* to 0.05."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train StarSpace embeddings for unigrams on the train dataset. You don't need to change the format of the input data. Just don't forget to use prepared version of the training data. \n",
    "\n",
    "If you follow the instruction, the training process will take about 1 hour. The size of the embeddings' dictionary should be approximately 100 000 (number of lines in the result file). If you got significantly more than this number, try to check all the instructions above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments: \n",
      "lr: 0.05\n",
      "dim: 100\n",
      "epoch: 5\n",
      "maxTrainTime: 8640000\n",
      "saveEveryEpoch: 0\n",
      "loss: hinge\n",
      "margin: 0.05\n",
      "similarity: cosine\n",
      "maxNegSamples: 10\n",
      "negSearchLimit: 10\n",
      "thread: 10\n",
      "minCount: 2\n",
      "minCountLabel: 1\n",
      "label: __label__\n",
      "ngrams: 1\n",
      "bucket: 2000000\n",
      "adagrad: 1\n",
      "trainMode: 3\n",
      "fileFormat: labelDoc\n",
      "normalizeText: 0\n",
      "dropoutLHS: 0\n",
      "dropoutRHS: 0\n",
      "Start to initialize starspace model.\n",
      "Build dict from input file : data/train_prepared.tsv\n",
      "Read 12M words\n",
      "Number of words in dictionary:  110503\n",
      "Number of labels in dictionary: 0\n",
      "Loading data from file : data/train_prepared.tsv\n",
      "Total number of examples loaded : 999624\n",
      "Initialized model weights. Model size :\n",
      "matrix : 110503 100\n",
      "Training epoch 0: 0.05 0.01\n",
      "Epoch: 100.0%  lr: 0.040000  loss: 0.009785  eta: 0h6m  tot: 0h1m33s  (20.0%)%  lr: 0.049970  loss: 0.060755  eta: 0h13m  tot: 0h0m0s  (0.1%)s  (0.4%)2.9%  lr: 0.049780  loss: 0.037071  eta: 0h8m  tot: 0h0m3s  (0.6%)8.7%  lr: 0.049129  loss: 0.025551  eta: 0h7m  tot: 0h0m8s  (1.7%)9.5%  lr: 0.049059  loss: 0.024548  eta: 0h7m  tot: 0h0m9s  (1.9%)10.9%  lr: 0.048919  loss: 0.023386  eta: 0h8m  tot: 0h0m10s  (2.2%)%)26.7%  lr: 0.047307  loss: 0.016264  eta: 0h7m  tot: 0h0m26s  (5.3%)27.0%  lr: 0.047277  loss: 0.016188  eta: 0h7m  tot: 0h0m26s  (5.4%)27.2%  lr: 0.047237  loss: 0.016143  eta: 0h7m  tot: 0h0m26s  (5.4%)28.8%  lr: 0.047127  loss: 0.015831  eta: 0h7m  tot: 0h0m28s  (5.8%)29.3%  lr: 0.047097  loss: 0.015715  eta: 0h7m  tot: 0h0m28s  (5.9%)29.8%  lr: 0.047037  loss: 0.015595  eta: 0h7m  tot: 0h0m29s  (6.0%)6.4%)  loss: 0.015166  eta: 0h7m  tot: 0h0m32s  (6.4%)32.1%  lr: 0.046817  loss: 0.015150  eta: 0h7m  tot: 0h0m32s  (6.4%)33.2%  lr: 0.046647  loss: 0.014927  eta: 0h7m  tot: 0h0m33s  (6.6%)34.7%  lr: 0.046547  loss: 0.014611  eta: 0h7m  tot: 0h0m34s  (6.9%)36.8%  lr: 0.046226  loss: 0.014305  eta: 0h7m  tot: 0h0m36s  (7.4%)  eta: 0h7m  tot: 0h0m37s  (7.4%)41.5%  lr: 0.045806  loss: 0.013671  eta: 0h7m  tot: 0h0m40s  (8.3%)  tot: 0h0m41s  (8.4%)46.9%  lr: 0.045205  loss: 0.012994  eta: 0h7m  tot: 0h0m46s  (9.4%)52.2%  lr: 0.044685  loss: 0.012424  eta: 0h7m  tot: 0h0m51s  (10.4%)53.8%  lr: 0.044535  loss: 0.012275  eta: 0h7m  tot: 0h0m52s  (10.8%)m  tot: 0h0m59s  (12.3%)65.8%  lr: 0.043163  loss: 0.011403  eta: 0h7m  tot: 0h1m3s  (13.2%)67.7%  lr: 0.043033  loss: 0.011285  eta: 0h6m  tot: 0h1m5s  (13.5%)68.2%  lr: 0.042973  loss: 0.011263  eta: 0h6m  tot: 0h1m5s  (13.6%)71.4%  lr: 0.042693  loss: 0.011086  eta: 0h6m  tot: 0h1m8s  (14.3%)74.1%  lr: 0.042463  loss: 0.010921  eta: 0h6m  tot: 0h1m11s  (14.8%)80.1%  lr: 0.041822  loss: 0.010578  eta: 0h6m  tot: 0h1m16s  (16.0%)83.0%  lr: 0.041422  loss: 0.010450  eta: 0h6m  tot: 0h1m19s  (16.6%)86.3%  lr: 0.040951  loss: 0.010304  eta: 0h6m  tot: 0h1m22s  (17.3%)%)87.9%  lr: 0.040781  loss: 0.010220  eta: 0h6m  tot: 0h1m24s  (17.6%)89.6%  lr: 0.040581  loss: 0.010157  eta: 0h6m  tot: 0h1m25s  (17.9%)91.7%  lr: 0.040431  loss: 0.010077  eta: 0h6m  tot: 0h1m27s  (18.3%)92.9%  lr: 0.040280  loss: 0.010039  eta: 0h6m  tot: 0h1m29s  (18.6%)\n",
      " ---+++                Epoch    0 Train error : 0.00992494 +++--- ���\n",
      "Training epoch 1: 0.04 0.01\n",
      "Epoch: 100.0%  lr: 0.030000  loss: 0.002633  eta: 0h4m  tot: 0h3m1s  (40.0%).4%  lr: 0.039990  loss: 0.002720  eta: 0h7m  tot: 0h1m33s  (20.1%)1.2%  lr: 0.039920  loss: 0.002793  eta: 0h6m  tot: 0h1m34s  (20.2%)  loss: 0.002476  eta: 0h6m  tot: 0h1m36s  (20.6%)5.9%  lr: 0.039439  loss: 0.002472  eta: 0h6m  tot: 0h1m39s  (21.2%)6.3%  lr: 0.039429  loss: 0.002471  eta: 0h6m  tot: 0h1m39s  (21.3%)7.6%  lr: 0.039289  loss: 0.002509  eta: 0h6m  tot: 0h1m40s  (21.5%)14.0%  lr: 0.038699  loss: 0.002554  eta: 0h5m  tot: 0h1m46s  (22.8%)15.2%  lr: 0.038569  loss: 0.002562  eta: 0h5m  tot: 0h1m47s  (23.0%)19.1%  lr: 0.038188  loss: 0.002569  eta: 0h5m  tot: 0h1m50s  (23.8%)21.0%  lr: 0.038008  loss: 0.002538  eta: 0h5m  tot: 0h1m52s  (24.2%)21.1%  lr: 0.037988  loss: 0.002532  eta: 0h5m  tot: 0h1m52s  (24.2%)21.7%  lr: 0.037938  loss: 0.002519  eta: 0h5m  tot: 0h1m53s  (24.3%)%  lr: 0.037858  loss: 0.002513  eta: 0h5m  tot: 0h1m54s  (24.5%)22.7%  lr: 0.037828  loss: 0.002514  eta: 0h5m  tot: 0h1m54s  (24.5%)23.2%  lr: 0.037748  loss: 0.002502  eta: 0h5m  tot: 0h1m55s  (24.6%)23.3%  lr: 0.037748  loss: 0.002518  eta: 0h5m  tot: 0h1m55s  (24.7%)23.9%  lr: 0.037628  loss: 0.002514  eta: 0h5m  tot: 0h1m55s  (24.8%)%  lr: 0.037417  loss: 0.002536  eta: 0h5m  tot: 0h1m57s  (25.1%)27.5%  lr: 0.037177  loss: 0.002541  eta: 0h5m  tot: 0h1m59s  (25.5%)37.6%  lr: 0.036206  loss: 0.002557  eta: 0h5m  tot: 0h2m8s  (27.5%)0.036116  loss: 0.002566  eta: 0h5m  tot: 0h2m8s  (27.7%)40.6%  lr: 0.035796  loss: 0.002565  eta: 0h5m  tot: 0h2m11s  (28.1%)  lr: 0.035245  loss: 0.002598  eta: 0h5m  tot: 0h2m15s  (29.1%)  eta: 0h5m  tot: 0h2m16s  (29.2%)47.5%  lr: 0.035095  loss: 0.002610  eta: 0h5m  tot: 0h2m17s  (29.5%)55.1%  lr: 0.034495  loss: 0.002606  eta: 0h5m  tot: 0h2m23s  (31.0%)60.7%  lr: 0.033844  loss: 0.002604  eta: 0h5m  tot: 0h2m28s  (32.1%)63.4%  lr: 0.033634  loss: 0.002595  eta: 0h5m  tot: 0h2m30s  (32.7%)66.8%  lr: 0.033223  loss: 0.002602  eta: 0h5m  tot: 0h2m33s  (33.4%)71.3%  lr: 0.032703  loss: 0.002596  eta: 0h4m  tot: 0h2m38s  (34.3%)73.0%  lr: 0.032583  loss: 0.002599  eta: 0h4m  tot: 0h2m39s  (34.6%)74.3%  lr: 0.032393  loss: 0.002592  eta: 0h4m  tot: 0h2m40s  (34.9%)78.2%  lr: 0.031932  loss: 0.002605  eta: 0h4m  tot: 0h2m44s  (35.6%)80.5%  lr: 0.031692  loss: 0.002613  eta: 0h4m  tot: 0h2m46s  (36.1%)82.6%  lr: 0.031462  loss: 0.002616  eta: 0h4m  tot: 0h2m47s  (36.5%)82.7%  lr: 0.031462  loss: 0.002616  eta: 0h4m  tot: 0h2m48s  (36.5%)88.3%  lr: 0.030891  loss: 0.002625  eta: 0h4m  tot: 0h2m52s  (37.7%)92.7%  lr: 0.030511  loss: 0.002622  eta: 0h4m  tot: 0h2m56s  (38.5%)\n",
      " ---+++                Epoch    1 Train error : 0.00266542 +++--- ���\n",
      "Training epoch 2: 0.03 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100.0%  lr: 0.020000  loss: 0.001816  eta: 0h2m  tot: 0h4m30s  (60.0%)%  lr: 0.029930  loss: 0.001981  eta: 0h4m  tot: 0h3m2s  (40.1%)4.8%  lr: 0.029439  loss: 0.001949  eta: 0h4m  tot: 0h3m5s  (41.0%)6.1%  lr: 0.029389  loss: 0.001886  eta: 0h4m  tot: 0h3m6s  (41.2%)7.4%  lr: 0.029209  loss: 0.001935  eta: 0h4m  tot: 0h3m8s  (41.5%)10.0%  lr: 0.029019  loss: 0.001930  eta: 0h4m  tot: 0h3m10s  (42.0%)%)23.5%  lr: 0.027488  loss: 0.001836  eta: 0h4m  tot: 0h3m22s  (44.7%)24.6%  lr: 0.027367  loss: 0.001830  eta: 0h4m  tot: 0h3m23s  (44.9%)26.5%  lr: 0.027177  loss: 0.001825  eta: 0h4m  tot: 0h3m25s  (45.3%)33.6%  lr: 0.026346  loss: 0.001835  eta: 0h4m  tot: 0h3m32s  (46.7%)35.8%  lr: 0.026206  loss: 0.001833  eta: 0h3m  tot: 0h3m34s  (47.2%)39.3%  lr: 0.025876  loss: 0.001832  eta: 0h3m  tot: 0h3m37s  (47.9%)39.6%  lr: 0.025846  loss: 0.001833  eta: 0h3m  tot: 0h3m37s  (47.9%)43.0%  lr: 0.025546  loss: 0.001832  eta: 0h3m  tot: 0h3m40s  (48.6%)46.8%  lr: 0.025235  loss: 0.001816  eta: 0h3m  tot: 0h3m43s  (49.4%) (49.6%)51.3%  lr: 0.024765  loss: 0.001823  eta: 0h3m  tot: 0h3m47s  (50.3%)52.4%  lr: 0.024685  loss: 0.001829  eta: 0h3m  tot: 0h3m48s  (50.5%)56.4%  lr: 0.024374  loss: 0.001829  eta: 0h3m  tot: 0h3m51s  (51.3%)57.4%  lr: 0.024284  loss: 0.001830  eta: 0h3m  tot: 0h3m52s  (51.5%)59.8%  lr: 0.024114  loss: 0.001834  eta: 0h3m  tot: 0h3m54s  (52.0%)62.8%  lr: 0.023784  loss: 0.001831  eta: 0h3m  tot: 0h3m57s  (52.6%)65.9%  lr: 0.023353  loss: 0.001820  eta: 0h3m  tot: 0h4m0s  (53.2%)68.0%  lr: 0.023183  loss: 0.001816  eta: 0h3m  tot: 0h4m2s  (53.6%)69.4%  lr: 0.023003  loss: 0.001825  eta: 0h3m  tot: 0h4m3s  (53.9%)69.8%  lr: 0.022973  loss: 0.001824  eta: 0h3m  tot: 0h4m3s  (54.0%)70.8%  lr: 0.022853  loss: 0.001827  eta: 0h3m  tot: 0h4m4s  (54.2%)70.9%  lr: 0.022853  loss: 0.001826  eta: 0h3m  tot: 0h4m5s  (54.2%)71.5%  lr: 0.022783  loss: 0.001824  eta: 0h3m  tot: 0h4m5s  (54.3%)72.0%  lr: 0.022743  loss: 0.001822  eta: 0h3m  tot: 0h4m6s  (54.4%)%  lr: 0.022583  loss: 0.001813  eta: 0h3m  tot: 0h4m7s  (54.6%)73.5%  lr: 0.022533  loss: 0.001811  eta: 0h3m  tot: 0h4m7s  (54.7%)74.0%  lr: 0.022483  loss: 0.001809  eta: 0h3m  tot: 0h4m8s  (54.8%)80.7%  lr: 0.021852  loss: 0.001818  eta: 0h3m  tot: 0h4m14s  (56.1%)81.5%  lr: 0.021792  loss: 0.001821  eta: 0h3m  tot: 0h4m15s  (56.3%)81.6%  lr: 0.021762  loss: 0.001821  eta: 0h3m  tot: 0h4m15s  (56.3%)85.3%  lr: 0.021382  loss: 0.001829  eta: 0h3m  tot: 0h4m18s  (57.1%)85.4%  lr: 0.021372  loss: 0.001828  eta: 0h3m  tot: 0h4m18s  (57.1%)%  lr: 0.021281  loss: 0.001822  eta: 0h3m  tot: 0h4m19s  (57.2%)86.8%  lr: 0.021201  loss: 0.001821  eta: 0h3m  tot: 0h4m20s  (57.4%)%  lr: 0.020280  loss: 0.001827  eta: 0h3m  tot: 0h4m27s  (58.9%)\n",
      " ---+++                Epoch    2 Train error : 0.00183841 +++--- ���\n",
      "Training epoch 3: 0.02 0.01\n",
      "Epoch: 100.0%  lr: 0.010000  loss: 0.001449  eta: 0h1m  tot: 0h5m59s  (80.0%)7%  lr: 0.019299  loss: 0.001468  eta: 0h2m  tot: 0h4m35s  (61.3%)8.6%  lr: 0.019129  loss: 0.001377  eta: 0h2m  tot: 0h4m37s  (61.7%)10.2%  lr: 0.018949  loss: 0.001381  eta: 0h2m  tot: 0h4m38s  (62.0%)11.0%  lr: 0.018929  loss: 0.001372  eta: 0h2m  tot: 0h4m39s  (62.2%)13.9%  lr: 0.018799  loss: 0.001454  eta: 0h2m  tot: 0h4m41s  (62.8%)16.3%  lr: 0.018539  loss: 0.001474  eta: 0h2m  tot: 0h4m44s  (63.3%)  tot: 0h4m46s  (63.7%)20.3%  lr: 0.018108  loss: 0.001488  eta: 0h2m  tot: 0h4m47s  (64.1%)24.1%  lr: 0.017688  loss: 0.001482  eta: 0h2m  tot: 0h4m51s  (64.8%)24.6%  lr: 0.017638  loss: 0.001474  eta: 0h2m  tot: 0h4m51s  (64.9%)24.9%  lr: 0.017568  loss: 0.001477  eta: 0h2m  tot: 0h4m51s  (65.0%) (65.8%)35.5%  lr: 0.016537  loss: 0.001480  eta: 0h2m  tot: 0h5m1s  (67.1%)41.5%  lr: 0.015906  loss: 0.001490  eta: 0h2m  tot: 0h5m6s  (68.3%)42.0%  lr: 0.015856  loss: 0.001492  eta: 0h2m  tot: 0h5m6s  (68.4%)43.4%  lr: 0.015646  loss: 0.001493  eta: 0h2m  tot: 0h5m8s  (68.7%)44.5%  lr: 0.015506  loss: 0.001483  eta: 0h2m  tot: 0h5m9s  (68.9%)44.7%  lr: 0.015456  loss: 0.001483  eta: 0h2m  tot: 0h5m10s  (68.9%)46.5%  lr: 0.015265  loss: 0.001477  eta: 0h2m  tot: 0h5m11s  (69.3%)50.1%  lr: 0.014845  loss: 0.001455  eta: 0h2m  tot: 0h5m14s  (70.0%)50.3%  lr: 0.014845  loss: 0.001455  eta: 0h2m  tot: 0h5m14s  (70.1%)50.7%  lr: 0.014795  loss: 0.001453  eta: 0h2m  tot: 0h5m14s  (70.1%)53.0%  lr: 0.014535  loss: 0.001459  eta: 0h2m  tot: 0h5m17s  (70.6%)54.6%  lr: 0.014304  loss: 0.001457  eta: 0h2m  tot: 0h5m18s  (70.9%)55.4%  lr: 0.014184  loss: 0.001459  eta: 0h2m  tot: 0h5m19s  (71.1%)58.9%  lr: 0.013874  loss: 0.001448  eta: 0h2m  tot: 0h5m22s  (71.8%)59.6%  lr: 0.013744  loss: 0.001447  eta: 0h2m  tot: 0h5m23s  (71.9%)  loss: 0.001454  eta: 0h2m  tot: 0h5m25s  (72.2%)63.0%  lr: 0.013484  loss: 0.001446  eta: 0h2m  tot: 0h5m26s  (72.6%)65.8%  lr: 0.013163  loss: 0.001441  eta: 0h2m  tot: 0h5m29s  (73.2%)66.8%  lr: 0.013073  loss: 0.001443  eta: 0h2m  tot: 0h5m30s  (73.4%)70.4%  lr: 0.012503  loss: 0.001459  eta: 0h1m  tot: 0h5m33s  (74.1%)72.8%  lr: 0.012272  loss: 0.001459  eta: 0h1m  tot: 0h5m35s  (74.6%)74.2%  lr: 0.012212  loss: 0.001459  eta: 0h1m  tot: 0h5m37s  (74.8%)74.6%  lr: 0.012172  loss: 0.001457  eta: 0h1m  tot: 0h5m37s  (74.9%)76.0%  lr: 0.012032  loss: 0.001456  eta: 0h1m  tot: 0h5m39s  (75.2%)77.5%  lr: 0.011952  loss: 0.001464  eta: 0h1m  tot: 0h5m40s  (75.5%)80.4%  lr: 0.011642  loss: 0.001459  eta: 0h1m  tot: 0h5m43s  (76.1%)81.0%  lr: 0.011552  loss: 0.001458  eta: 0h1m  tot: 0h5m43s  (76.2%)81.2%  lr: 0.011512  loss: 0.001458  eta: 0h1m  tot: 0h5m43s  (76.2%)81.7%  lr: 0.011432  loss: 0.001459  eta: 0h1m  tot: 0h5m44s  (76.3%)%  lr: 0.011402  loss: 0.001457  eta: 0h1m  tot: 0h5m44s  (76.4%)85.0%  lr: 0.011091  loss: 0.001455  eta: 0h1m  tot: 0h5m47s  (77.0%)86.0%  lr: 0.010991  loss: 0.001454  eta: 0h1m  tot: 0h5m48s  (77.2%)89.8%  lr: 0.010661  loss: 0.001460  eta: 0h1m  tot: 0h5m51s  (78.0%)\n",
      " ---+++                Epoch    3 Train error : 0.00148068 +++--- ���\n",
      "Training epoch 4: 0.01 0.01\n",
      "Epoch: 100.0%  lr: 0.000000  loss: 0.001283  eta: <1min   tot: 0h7m28s  (100.0%)%  lr: 0.008859  loss: 0.001249  eta: 0h1m  tot: 0h6m10s  (82.3%)12.7%  lr: 0.008739  loss: 0.001240  eta: 0h1m  tot: 0h6m11s  (82.5%)13.3%  lr: 0.008689  loss: 0.001233  eta: 0h1m  tot: 0h6m12s  (82.7%)16.7%  lr: 0.008338  loss: 0.001221  eta: 0h1m  tot: 0h6m14s  (83.3%)23.6%  lr: 0.007588  loss: 0.001256  eta: 0h1m  tot: 0h6m21s  (84.7%)23.7%  lr: 0.007578  loss: 0.001255  eta: 0h1m  tot: 0h6m21s  (84.7%)27.7%  lr: 0.007197  loss: 0.001245  eta: 0h1m  tot: 0h6m24s  (85.5%)28.0%  lr: 0.007107  loss: 0.001244  eta: 0h1m  tot: 0h6m24s  (85.6%)m  tot: 0h6m25s  (85.7%)%  lr: 0.007037  loss: 0.001258  eta: 0h1m  tot: 0h6m25s  (85.7%)33.3%  lr: 0.006547  loss: 0.001237  eta: 0h1m  tot: 0h6m29s  (86.7%)42.2%  lr: 0.005716  loss: 0.001254  eta: <1min   tot: 0h6m37s  (88.4%)42.4%  lr: 0.005706  loss: 0.001256  eta: <1min   tot: 0h6m37s  (88.5%)51.7%  lr: 0.004755  loss: 0.001247  eta: <1min   tot: 0h6m45s  (90.3%)0.001252  eta: <1min   tot: 0h6m45s  (90.4%)54.2%  lr: 0.004475  loss: 0.001256  eta: <1min   tot: 0h6m47s  (90.8%)61.1%  lr: 0.003754  loss: 0.001265  eta: <1min   tot: 0h6m54s  (92.2%)70.7%  lr: 0.002843  loss: 0.001286  eta: <1min   tot: 0h7m2s  (94.1%)76.9%  lr: 0.002202  loss: 0.001287  eta: <1min   tot: 0h7m8s  (95.4%)7m9s  (95.6%)m9s  (95.7%)78.8%  lr: 0.001952  loss: 0.001291  eta: <1min   tot: 0h7m9s  (95.8%)83.6%  lr: 0.001411  loss: 0.001283  eta: <1min   tot: 0h7m14s  (96.7%)86.0%  lr: 0.001221  loss: 0.001292  eta: <1min   tot: 0h7m16s  (97.2%)86.7%  lr: 0.001121  loss: 0.001296  eta: <1min   tot: 0h7m17s  (97.3%)90.8%  lr: 0.000661  loss: 0.001291  eta: <1min   tot: 0h7m21s  (98.2%)91.5%  lr: 0.000621  loss: 0.001289  eta: <1min   tot: 0h7m22s  (98.3%)92.8%  lr: 0.000491  loss: 0.001288  eta: <1min   tot: 0h7m23s  (98.6%)92.9%  lr: 0.000491  loss: 0.001288  eta: <1min   tot: 0h7m23s  (98.6%)\n",
      " ---+++                Epoch    4 Train error : 0.00129981 +++--- ���\n",
      "Saving model to file : week3model\n",
      "Saving model in tsv format : week3model.tsv\n"
     ]
    }
   ],
   "source": [
    "######### TRAINING HAPPENING HERE #############\n",
    "!starspace train -trainFile data/train_prepared.tsv -trainMode 3 -adagrad true -ngrams 1 -epoch 5  \\\n",
    " -dim 100 -similarity cosine -model week3model -minCount 2 -verbose true -fileFormat labelDoc \\\n",
    " -negSearchLimit 10 -lr 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "And now we can compare the new embeddings with the previous ones. You can find trained word vectors in the file *[model_file_name].tsv*. Upload the embeddings from StarSpace into a dict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "java\n",
      "\n"
     ]
    }
   ],
   "source": [
    "starspace_embeddings = dict()\n",
    "for line in open('week3model.tsv', encoding='utf-8'):\n",
    "    q, *ex = line.strip().split('\\t')\n",
    "    if len(ex) < 100:\n",
    "        print(line)\n",
    "        break\n",
    "    starspace_embeddings[q] = np.array(ex, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('data/train_prepared.tsv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'html 5 canvas javascript use making interactive drawing tool\\tevent handling geometries threejs\\n'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = next(f)\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file\\t0.0208124\\t0.0344047\\t-0.0344049\\t0.0657013\\t0.020469\\t-0.0194569\\t-0.0400735\\t0.0586822\\t0.0689199\\t-0.026807\\t0.0304413\\t-0.0525206\\t-0.066155\\t-0.00605997\\t0.0316914\\t-0.0107329\\t0.0349454\\t-0.0328102\\t0.0136964\\t0.0315276\\t-0.0181975\\t0.00870207\\t0.0285195\\t-0.0397131\\t-0.0215149\\t0.0256014\\t-0.0295706\\t0.0409087\\t0.0398968\\t-0.0052346\\t-0.00922728\\t-0.0258903\\t-0.00345767\\t-0.0104773\\t0.0423385\\t0.0317324\\t0.0153624\\t-0.0249137\\t-0.00766188\\t0.0154943\\t-0.0506855\\t-0.0440863\\t0.00276399\\t0.0335753\\t0.0321486\\t-0.036046\\t-0.0128471\\t0.000851746\\t0.103289\\t0.0815258\\t-0.016628\\t0.00627557\\t0.0394692\\t-0.00855764\\t0.0251038\\t0.0451494\\t-0.10446\\t-0.00516351\\t0.0206225\\t0.038589\\t-0.00078419\\t-0.0135166\\t-0.039732\\t-0.0781202\\t-0.0166548\\t0.0201908\\t-0.013822\\t-0.0227522\\t-0.0191417\\t0.0288483\\t-0.062627\\t-0.00626662\\t-0.00877586\\t0.0235134\\t0.022744\\t-0.00708179\\t0.0160009\\t0.033836\\t0.0167684\\t0.0520947\\t-0.0625077\\t-0.0771654\\t-0.0530052\\t0.00936241\\t-0.0110048\\t-0.0401759\\t-0.0574281\\t0.0160362\\t-0.0446755\\t-0.0140575\\t-0.011717\\t-0.0610744\\t-0.0254684\\t0.00992614\\t-0.00101027\\t-0.0144041\\t-0.0129862\\t0.0120083\\t-0.0909323\\t0.0209905\\n'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = next(f)\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'android\\t0.0330639\\t-0.0195711\\t0.00182071\\t0.0217052\\t-0.0280802\\t0.0191014\\t0.0480053\\t0.0307314\\t-0.0966976\\t0.0716527\\t0.00957561\\t-0.0235741\\t-0.0710098\\t0.0300581\\t-0.0273113\\t0.0698121\\t0.00717061\\t-0.0402271\\t-0.0616039\\t-0.0196532\\t0.032136\\t0.0669243\\t0.0257606\\t-0.0207143\\t0.00776075\\t0.0356336\\t0.0612106\\t0.0465131\\t-0.020977\\t-0.0376891\\t0.0266019\\t-0.0491257\\t-0.0778359\\t0.059647\\t0.009146\\t-0.0670566\\t-0.055453\\t-0.0202815\\t-0.0603593\\t0.0123468\\t-0.0191864\\t-0.00873635\\t0.0222912\\t0.0288145\\t-0.0061137\\t0.0710339\\t0.0722912\\t0.113893\\t0.0501821\\t-0.0646072\\t-0.0414661\\t-0.0943278\\t0.0165119\\t0.0348931\\t0.0473046\\t-0.0202202\\t0.0243119\\t0.00786317\\t-0.0929114\\t0.0286816\\t-0.0286559\\t-0.0318042\\t0.0245244\\t0.0365115\\t0.0395255\\t-0.00529468\\t0.0813587\\t0.0186529\\t0.000917076\\t-0.0298803\\t0.000398173\\t-0.0392098\\t-0.0263845\\t0.0131706\\t0.0537836\\t0.0200519\\t-0.0701383\\t-0.0101738\\t0.0120453\\t0.0480176\\t-0.0641946\\t-0.00757235\\t-0.049224\\t0.0401026\\t-0.0438248\\t0.144037\\t-0.0775279\\t-0.0149693\\t-0.0407998\\t-0.0483144\\t-0.0193669\\t0.0182757\\t-0.00815308\\t-0.0586733\\t-0.00458351\\t-0.0481455\\t-0.0061529\\t0.00775931\\t0.00718323\\t0.068588\\n'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = next(f)\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'get\\t-0.0116774\\t0.070582\\t0.0158432\\t0.0193376\\t0.00104479\\t-0.0285625\\t-0.000146783\\t0.0211616\\t-0.0207828\\t0.00693713\\t0.0356723\\t0.0382138\\t0.00326956\\t0.0465548\\t-0.0141246\\t0.00279946\\t0.039973\\t-0.0203557\\t-0.00645734\\t0.0366549\\t0.0101151\\t0.0140132\\t-0.0173132\\t-0.0129756\\t0.0287837\\t0.00062601\\t0.0357466\\t0.0220668\\t0.0670331\\t-0.000237744\\t-0.0306797\\t-0.0143791\\t0.0121673\\t-0.0503801\\t-0.0233542\\t0.0275014\\t-0.0421322\\t-0.0125225\\t-0.000735657\\t-0.00621687\\t0.000296384\\t0.0400397\\t0.00707109\\t0.0324031\\t0.027234\\t-0.0266162\\t0.0617837\\t0.0229032\\t-0.031907\\t-0.0233232\\t0.0145715\\t0.0363197\\t-0.0344193\\t0.0377439\\t0.0313878\\t-0.0110791\\t0.00981241\\t0.0135075\\t-0.0133405\\t0.0864139\\t0.0330966\\t-0.01392\\t0.0054076\\t-0.0175608\\t-0.0301374\\t-0.0275253\\t0.00403108\\t-0.0828083\\t-0.0251905\\t-0.019045\\t0.00576671\\t0.0259508\\t-0.0323469\\t0.0115187\\t0.00232662\\t-0.00498826\\t0.0169493\\t0.0160852\\t-0.0125209\\t0.00984904\\t-0.0721773\\t0.0248424\\t0.0348976\\t-0.0247185\\t0.0244611\\t0.0134911\\t-0.0271174\\t-0.00759316\\t0.0118168\\t-0.0195176\\t0.0159759\\t0.0180708\\t0.0155761\\t-0.0261586\\t-0.0199929\\t-0.00777197\\t0.0183507\\t0.0545732\\t0.0330588\\t-0.0513435\\n'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = next(f)\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'use\\t0.0137033\\t-0.00887483\\t0.035095\\t-0.0105588\\t-0.00342779\\t-0.00230318\\t0.00344651\\t-0.0142561\\t-0.0146811\\t-0.00927209\\t0.0188775\\t-0.00272003\\t-0.00681423\\t0.0111948\\t-0.0132769\\t0.0133479\\t-0.0106998\\t0.0431333\\t0.0176371\\t0.0267702\\t0.00272782\\t0.0218289\\t0.0388948\\t-0.0245845\\t-0.0281702\\t-0.0104557\\t0.0186122\\t0.00591279\\t0.0474088\\t-0.0103159\\t0.000377793\\t0.0343758\\t-0.0135752\\t0.0174126\\t-0.0292738\\t0.00164773\\t0.0133836\\t0.0180813\\t0.0275989\\t0.00668033\\t-0.0185096\\t0.0116124\\t-0.0437978\\t-0.0284374\\t-0.0243909\\t-0.0202719\\t-0.0266302\\t0.0516295\\t-0.0191912\\t-0.0283222\\t-0.0131145\\t-0.0172813\\t0.0131034\\t-0.00138217\\t0.00603364\\t0.0296496\\t-0.0112317\\t-0.00407906\\t0.012194\\t-0.0107108\\t-0.0242655\\t0.0234042\\t0.0301197\\t-0.00937211\\t0.0153755\\t-2.33994e-05\\t-0.0218179\\t-0.0737267\\t0.0408734\\t-0.0379083\\t0.00833017\\t-0.000499396\\t-0.0245212\\t-0.00939275\\t0.0146459\\t-0.0345721\\t0.033891\\t0.035432\\t0.0220838\\t-0.0139029\\t-0.00342063\\t-0.00898115\\t-0.0184814\\t0.0412357\\t0.00555189\\t-0.0236022\\t0.0204502\\t0.000341246\\t0.00232225\\t0.00163839\\t-0.0027443\\t-0.00297432\\t0.00748463\\t-0.010532\\t-0.000439386\\t-0.0390829\\t-0.0173321\\t-0.00475598\\t0.0203169\\t-0.0013947\\n'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = next(f)\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'java\\t0.0386055\\t0.0201119\\t0.0328729\\t-0.0957672\\t0.0114957\\t-0.0278843\\t-0.0767195\\t0.0291077\\t-0.0134255\\t0.131172\\t-0.0105205\\t-0.0185705\\t-0.0263503\\t0.0163467\\t-0.0312587\\t-0.0166329\\t0.0212224\\t0.0568228\\t-0.00271723\\t-0.0320586\\t0.0531453\\t-0.0392828\\t-0.0165459\\t-0.101559\\t-0.0284917\\t0.0374062\\t-0.0142094\\t-0.0104406\\t-0.0789606\\t-0.0511784\\t0.0373867\\t-0.0766869\\t0.00306198\\t-0.00580975\\t-0.0493982\\t0.0426011\\t-0.0667633\\t-0.0393443\\t-0.0033275\\t-0.0215191\\t-0.00194586\\t0.0408036\\t0.0276181\\t0.0207256\\t-0.0435252\\t-0.0384805\\t0.0480162\\t-0.0192185\\t0.0592976\\t-0.00716955\\t-0.0445932\\t-0.0596022\\t0.0773285\\t0.0632403\\t0.0858511\\t-0.009515\\t0.0503408\\t-0.00910577\\t-0.0754992\\t-0.021798\\t0.017973\\t0.0357145\\t0.0177712\\t0.00202541\\t-0.011262\\t-0.0295227\\t-0.0521981\\t0.0465615\\t-0.0203596\\t-0.0225617\\t-0.0901211\\t-0.0617219\\t-0.0669932\\t-0.0520492\\t-0.0156596\\t-0.0327914\\t-0.0794136\\t0.116028\\t-0.000526737\\t0.0454618\\t-0.0797828\\t-0.0497482\\t0.00378102\\t0.0569748\\t0.0192137\\t0.0195116\\t0.00423902\\t-0.0265121\\t-0.017859\\t-0.0545333\\t0.0228769\\t0.113021\\t0.00422223\\t0.0360156\\t0.00443181\\t0.129974\\t0.00943552\\t-0.00517584\\t-0.058534\\t0.00841443\\n'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = next(f)\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['java',\n",
       " '0.0386055',\n",
       " '0.0201119',\n",
       " '0.0328729',\n",
       " '-0.0957672',\n",
       " '0.0114957',\n",
       " '-0.0278843',\n",
       " '-0.0767195',\n",
       " '0.0291077',\n",
       " '-0.0134255',\n",
       " '0.131172',\n",
       " '-0.0105205',\n",
       " '-0.0185705',\n",
       " '-0.0263503',\n",
       " '0.0163467',\n",
       " '-0.0312587',\n",
       " '-0.0166329',\n",
       " '0.0212224',\n",
       " '0.0568228',\n",
       " '-0.00271723',\n",
       " '-0.0320586',\n",
       " '0.0531453',\n",
       " '-0.0392828',\n",
       " '-0.0165459',\n",
       " '-0.101559',\n",
       " '-0.0284917',\n",
       " '0.0374062',\n",
       " '-0.0142094',\n",
       " '-0.0104406',\n",
       " '-0.0789606',\n",
       " '-0.0511784',\n",
       " '0.0373867',\n",
       " '-0.0766869',\n",
       " '0.00306198',\n",
       " '-0.00580975',\n",
       " '-0.0493982',\n",
       " '0.0426011',\n",
       " '-0.0667633',\n",
       " '-0.0393443',\n",
       " '-0.0033275',\n",
       " '-0.0215191',\n",
       " '-0.00194586',\n",
       " '0.0408036',\n",
       " '0.0276181',\n",
       " '0.0207256',\n",
       " '-0.0435252',\n",
       " '-0.0384805',\n",
       " '0.0480162',\n",
       " '-0.0192185',\n",
       " '0.0592976',\n",
       " '-0.00716955',\n",
       " '-0.0445932',\n",
       " '-0.0596022',\n",
       " '0.0773285',\n",
       " '0.0632403',\n",
       " '0.0858511',\n",
       " '-0.009515',\n",
       " '0.0503408',\n",
       " '-0.00910577',\n",
       " '-0.0754992',\n",
       " '-0.021798',\n",
       " '0.017973',\n",
       " '0.0357145',\n",
       " '0.0177712',\n",
       " '0.00202541',\n",
       " '-0.011262',\n",
       " '-0.0295227',\n",
       " '-0.0521981',\n",
       " '0.0465615',\n",
       " '-0.0203596',\n",
       " '-0.0225617',\n",
       " '-0.0901211',\n",
       " '-0.0617219',\n",
       " '-0.0669932',\n",
       " '-0.0520492',\n",
       " '-0.0156596',\n",
       " '-0.0327914',\n",
       " '-0.0794136',\n",
       " '0.116028',\n",
       " '-0.000526737',\n",
       " '0.0454618',\n",
       " '-0.0797828',\n",
       " '-0.0497482',\n",
       " '0.00378102',\n",
       " '0.0569748',\n",
       " '0.0192137',\n",
       " '0.0195116',\n",
       " '0.00423902',\n",
       " '-0.0265121',\n",
       " '-0.017859',\n",
       " '-0.0545333',\n",
       " '0.0228769',\n",
       " '0.113021',\n",
       " '0.00422223',\n",
       " '0.0360156',\n",
       " '0.00443181',\n",
       " '0.129974',\n",
       " '0.00943552',\n",
       " '-0.00517584',\n",
       " '-0.058534',\n",
       " '0.00841443']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.strip().split('\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100,\n",
       " 99,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 99,\n",
       " 99,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 0,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 0,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 99,\n",
       " 0,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 99,\n",
       " 99,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 0,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 99,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 99,\n",
       " 0,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 99,\n",
       " 0,\n",
       " 99,\n",
       " 99,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 99,\n",
       " 100,\n",
       " 0,\n",
       " 0,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 99,\n",
       " 99,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 99,\n",
       " 100,\n",
       " 0,\n",
       " 99,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 0,\n",
       " 99,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 0,\n",
       " 99,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 99,\n",
       " 0,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 99,\n",
       " 0,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 0,\n",
       " 0,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 0,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 99,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 99,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 99,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 0,\n",
       " 0,\n",
       " 99,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 0,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 99,\n",
       " 99,\n",
       " 0,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 99,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 99,\n",
       " 99,\n",
       " 0,\n",
       " 99,\n",
       " 0,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 99,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 99,\n",
       " 99,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 99,\n",
       " 0,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 0,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 0,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 99,\n",
       " 99,\n",
       " 99,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 99,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 0,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 0,\n",
       " 0,\n",
       " 99,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 0,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 99,\n",
       " 0,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 0,\n",
       " 100,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 0,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 99,\n",
       " 99,\n",
       " 99,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 0,\n",
       " 100,\n",
       " 0,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 0,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 99,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 0,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 99,\n",
       " 99,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 99,\n",
       " 100,\n",
       " 0,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 99,\n",
       " 100,\n",
       " 0,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 99,\n",
       " 100,\n",
       " 99,\n",
       " 99,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 0,\n",
       " 99,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 0,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 0,\n",
       " 0,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 99,\n",
       " 100,\n",
       " 99,\n",
       " 99,\n",
       " 100,\n",
       " 0,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 0,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 99,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 0,\n",
       " 100,\n",
       " 99,\n",
       " 99,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 0,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 0,\n",
       " 99,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 0,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 99,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 0,\n",
       " 99,\n",
       " 99,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 0,\n",
       " 99,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 0,\n",
       " 0,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 99,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 99,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 99,\n",
       " 0,\n",
       " 100,\n",
       " 99,\n",
       " 99,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 99,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 99,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 99,\n",
       " 99,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 0,\n",
       " 99,\n",
       " 99,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 99,\n",
       " 0,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 99,\n",
       " 99,\n",
       " 0,\n",
       " 99,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 99,\n",
       " 99,\n",
       " 0,\n",
       " 99,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 0,\n",
       " 100,\n",
       " 99,\n",
       " 100,\n",
       " ...]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(e) for e in starspace_embeddings.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (100,) (0,) (100,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-b5fb095b89e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprepared_validation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mranks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrank_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstarspace_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mss_prepared_ranking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mranks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-fae229d37c2e>\u001b[0m in \u001b[0;36mrank_candidates\u001b[0;34m(question, candidates, embeddings, dim)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m######### YOUR CODE HERE #############\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m######################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mquestion_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquestion_to_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0msimilarity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion_embedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mquestion_to_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-c0500014b079>\u001b[0m in \u001b[0;36mquestion_to_vec\u001b[0;34m(question, embeddings, dim)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0membedding\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mfound_words\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0membedding\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (100,) (0,) (100,) "
     ]
    }
   ],
   "source": [
    "ss_prepared_ranking = []\n",
    "for line in prepared_validation:\n",
    "    q, *ex = line\n",
    "    ranks = rank_candidates(q, ex, starspace_embeddings, 100)\n",
    "    ss_prepared_ranking.append([r[0] for r in ranks].index(0) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k in [1, 5, 10, 100, 500, 1000]:\n",
    "    print(\"DCG@%4d: %.3f | Hits@%4d: %.3f\" % (k, dcg_score(ss_prepared_ranking, k), \n",
    "                                               k, hits_count(ss_prepared_ranking, k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to training for the particular task with the supervised data, you should expect to obtain a higher quality than for the previous approach. In additiion, despite the fact that StarSpace's trained vectors have a smaller dimension than word2vec's, it provides better results in this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5 (StarSpaceRanks).** For each question from prepared *test.tsv* submit the ranks of the candidates for trained representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "starspace_ranks_results = []\n",
    "prepared_test_data = ######### YOUR CODE HERE #############\n",
    "for line in open(prepared_test_data):\n",
    "    q, *ex = line.strip().split('\\t')\n",
    "    ranks = rank_candidates(q, ex, starspace_embeddings, 100)\n",
    "    ranked_candidates = [r[0] for r in ranks]\n",
    "    starspace_ranks_results.append([ranked_candidates.index(i) + 1 for i in range(len(ranked_candidates))])\n",
    "    \n",
    "grader.submit_tag('StarSpaceRanks', matrix_to_string(starspace_ranks_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please, **don't remove** the file with these embeddings because you will need them in the final project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authorization & Submission\n",
    "To submit assignment parts to Cousera platform, please, enter your e-mail and token into variables below. You can generate token on this programming assignment page. <b>Note:</b> Token expires 30 minutes after generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You want to submit these parts:\n",
      "Task Question2Vec: 0.01929389126598835\n",
      "-0.02872721292078495\n",
      "0.0460561104118824\n",
      "0.0852593332529068\n",
      "0.0243055559694767\n",
      "-0...\n",
      "Task HitsCount: 1.0\n",
      "0.5\n",
      "1.0\n",
      "0.5\n",
      "1.0\n",
      "0.3333333333333333\n",
      "0.6666666666666666\n",
      "1.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "1....\n",
      "Task DCGScore: 1.0\n",
      "0.5\n",
      "0.8154648767857288\n",
      "0.5\n",
      "0.8154648767857288\n",
      "0.3333333333333333\n",
      "0.5436432511904858\n",
      "0.7103099178...\n",
      "Task W2VTokenizedRanks: 95\t94\t7\t9\t64\t37\t32\t93\t24\t100\t98\t17\t60\t6\t97\t49\t70\t38\t42\t96\t30\t21\t2\t65\t67\t45\t27\t26\t57\t62\t11\t88\t56\t66\t7...\n",
      "Task StarSpaceRanks: ----------...\n"
     ]
    }
   ],
   "source": [
    "STUDENT_EMAIL = 'nomenou@gmail.com'\n",
    "STUDENT_TOKEN = 'I3JldM9ST4ZtlRtT'\n",
    "grader.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to submit these answers, run cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted to Coursera platform. See results on assignment page!\n"
     ]
    }
   ],
   "source": [
    "grader.submit(STUDENT_EMAIL, STUDENT_TOKEN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {
    "22b636da0ef941c2bd67b4e3a8611f87": {
     "views": []
    },
    "4cb6a50f7c2f432cabc8c3c43183a8cb": {
     "views": []
    },
    "6c5deffa43bb4102aba713967b49da12": {
     "views": [
      {
       "cell_index": 3
      }
     ]
    },
    "8842bcff5a4f4137a85aa57d30309167": {
     "views": []
    },
    "934ea4f36fed44c38a6813c0dc673c73": {
     "views": []
    },
    "e72fba378c504d70a6584dc51822895e": {
     "views": []
    }
   },
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
